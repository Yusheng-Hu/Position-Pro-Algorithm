name: Python Iter Performance Bench

on:
  push:
    branches: [ "main" ]
  workflow_dispatch:

# Shared concurrency group to force sequential execution and prevent Exit Code 128
concurrency:
  group: global-readme-sync
  cancel-in-progress: false

permissions:
  contents: write

jobs:
  python-bench:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0 # Full history required for rebase push

    - name: Set up PyPy 3.10
      uses: actions/setup-python@v5
      with:
        python-version: 'pypy-3.10'

    - name: Run Benchmark and Generate Report
      run: |
        cd python
        # 1. Run the benchmark script first
        python pp_iter.py > raw_data.tmp
        
        # 2. Use Python to generate the final formatted report to avoid awk/shell errors
        python3 -c "
        import subprocess, datetime, os

        # Get CPU Model
        try:
            cpu = subprocess.check_output('grep -m 1 \"model name\" /proc/cpuinfo', shell=True).decode().split(':')[1].strip()
        except:
            cpu = 'GitHub Actions Runner'

        # Get Timestamps
        now_utc = datetime.datetime.now(datetime.timezone.utc)
        now_bj = now_utc + datetime.timedelta(hours=8)
        time_str = f\"{now_utc.strftime('%a %b %d %H:%M:%S %Y')} UTC / {now_bj.strftime('%a %b %d %H:%M:%S %Y')} (UTC+8)\"

        # Build Report Content
        report = []
        report.append(f'#### Last Automated Run: {time_str}')
        report.append(f'**Environment: {cpu}**\n')
        report.append('| N | Total Permutations | Itertools (s) | Position Pure (s) | Speed-up |')
        report.append('|---|---|---|---|---|')

        # Read raw data and format into table rows
        if os.path.exists('raw_data.tmp'):
            with open('raw_data.tmp', 'r') as f:
                for line in f:
                    parts = line.split()
                    if len(parts) >= 5:
                        report.append(f'| {parts[0]} | {parts[1]} | {parts[2]} | {parts[3]} | {parts[4]} |')

        with open('bench_output.md', 'w', encoding='utf-8') as f:
            f.write('\n'.join(report))
        "
        # Export to Job Summary for quick check
        cat bench_output.md >> $GITHUB_STEP_SUMMARY

    - name: Inject into README.md
      shell: python
      run: |
        import os, sys
        start_marker = "[//]: # (PYTHON_PP_ITER_BENCHMARK_START)"
        end_marker = "[//]: # (PYTHON_PP_ITER_BENCHMARK_END)"

        with open("python/bench_output.md", "r", encoding="utf-8") as f:
            new_report = f.read().strip()
        with open("README.md", "r", encoding="utf-8") as f:
            readme = f.read()

        if start_marker in readme and end_marker in readme:
            prefix = readme.split(start_marker)[0].strip()
            suffix = readme.split(end_marker)[-1].strip()
            
            # Combine all elements into the final block
            content = f"### üêç Position Pure Iterator Performance (PyPy)\n\n{new_report}"
            
            with open("README.md", "w", encoding="utf-8") as f:
                f.write(f"{prefix}\n\n{start_marker}\n\n{content}\n\n{end_marker}\n\n{suffix}\n")
        else:
            print("Markers missing!")
            sys.exit(1)

    - name: Synchronized Git Push (Exit Code 128 Fix)
      run: |
        git config --global user.name "github-actions[bot]"
        git config --global user.email "github-actions[bot]@users.noreply.github.com"
        
        for i in {1..5}; do
          git add README.md
          if git commit -m "docs: sync python benchmarks [skip ci]"; then
            git pull --rebase origin main
            if git push; then exit 0; fi
          else
            exit 0 # No changes
          fi
          sleep 5
        done
        exit 1
